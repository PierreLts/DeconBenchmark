# DeconvBenchmarking pipeline
This pipeline aims at running and benchmarking deconvolution models.
The pipeline is build on the [tinnlab/DeconBenchmark](https://github.com/tinnlab/DeconBenchmark) package which unifies the running of 50 deconvolution models in one function.

Pierre Lorthiois 04/2025

## 0. Running Files
Here `.R` files should be run through their corresponding `.sh` launch scripts.

## 1. Overview
### 1.1. Installations
First intall libraries and packages running `install_r_lib.R`
### 1.2. Flow Chart
Here is a detailed flow chart of the repository to help you understand the link between files. I suggest focusing on the results you want to get or the files you want to run in a first place to keep it simple.
<img src="data/FlowChart.png" width="800" alt="Detailed workflow diagram of the deconvolution benchmarking process">

You can see that the project is divided in four main categories:<br> 
Data Preparation<br>
Deconvolution Run<br>
Benchmarking<br>
Debugging<br>
<img src="data/SimpleFlowChart.png" width="650" alt="Detailed workflow diagram of the deconvolution benchmarking process">

## 2. Data Preparation
To run the pipeline, we first need properly formated data.
### 2.1. Data Folder
| The folder should contain: | |
|-------------|------------|
| Single cell RNA-seq data: | "scRNA".rds   (our case: 4 batches "GFB...Seurat.rds") |
| Bulk RNA-seq data: | "bulkRNA".csv        (our case: "cleaned_feature_counts_matrix.csv") |
| Gene name mapping file: | mart_export.txt (Already in the repository) |

Running `merge_batches.R` will save "merged_batches.rds" which are the 4 batches of your choice merged (in our case the 4 batches "GFB...Seurat.rds")

### 2.2. Data Generation
The scripts in this section are in *scripts/data/*, and files are created at */generated_data*:
| `all_data.sh` launches: | |
|-------------|------------|
| **Script File** | **Output File** |
| `singleCellExpr_AB_generation.R` | "data_name"_singleCellExpr_AB.rda |
| `singleCellLabels_AB_generation.R` | "data_name"_singleCellLabels_AB.rda |
| `singleCellSubjects_AB_generation.R` | "data_name"_singleCellSubjects_AB.rda |
| | |
| `bulk_generation.R` | "data_name"_bulk.rda |
| `pseudobulk.R` | "data_name"_pseudobulk.rda |
| `bulk_randomizer.R` | "data_name"_bulk_random.rda |
| | |
| `GT_generation.R` | "data_name"_GT_proportions.rda |
| `GT_per_sample_generation.R` | "data_name"_GT_proportions_per_sample.rda |

You will need to adjust the paths of the scripts in `all_data.sh`, search for the 4 path sections to adjust: #### -SET PATHS ?/4- ####<br><br>
You want to **run it twice** because of a dependency issue: `GT_generation.R` uses /generated_data/"Job_name"_GT_per_sample.rda, generated by `GT_per_sample_genration.R`.<br><br>
.csv file versions are also saved for visualizations purpose.


## 3. Deconvolution Run
Once you have your data ready, you will be ready to run the deconvolution models of your choice on the data of your choice, running `all_deconv.sh`.<br><br>
Pay attention to the #### -DEFAULT PARAMETERS- #### section where you will choose your inputs, and to the section #### -SET PATHS- #### where you will adjust your scripts locations and etc.<br><br>

The scripts in this section are in *scripts/data/*, and files are created at */generated_data*.

## 4. Debugging
Now that yo have run 50+ different deconvolution models, you might realize that debugging is a challenge.
This section is dedicated to having a quick overview of the different errors that can occur.<br><br>

The script to generate and html error summary is scripts/debugg/`error_summary.sh`.<br>
Running this file will generate an interactive error sumary file called *error_summary.html* that you can open with your personnal browser.<br><br>

To run the script, pay attention to the #### -ADJUST PATHS & PARAMETERS ?/2- #### sections where you will adjust the specific data you ran on you want to debug, and the paths where jobs were mapped, etc.

## 5. Benchmarking
Now That we have generated results, we must interpret them. This section covers detailed benchmarking, that has to be run **in a first place**, to then run overall benchmarking.

### 5.1. Detailed Benchmarking
To generate detailed preformance tables and plot run scripts/benchmark/`all_benchmarks.sh`.
To run the script, pay attention to the #### -SET PARAMETERS- #### section, and to the #### -ADJUST PATHS- #### section.

### 5.2. Overall Benchmarking
Once you ran `all_benchmarks.sh`, you can generate general rdar plots to asses overall performances.

#### 5.2.1. Per Model Benchmarking
To generate a radar plot with every individula model's performance on a single plot, run /scripts/benchmark/`radar_models.sh`.<br>
This will generate a pdf plot in /benchmark_results/radar.<br><br>
To run it, pay attention to the #### -ADJUST PATHS ?/4- #### sections, and to the #### -SET PARAMETERS- #### section.

#### 5.2.1. Per Data Benchmarking
To generate a radar plot with overall preformance from a dataset to another (with model's performances averaged together), run /scripts/benchmark/`radar_compare.sh`.<br>
This will generate a pdf plot called /benchmark_results/radar/radar_comparision.pdf.<br><br>
To run it, pay attention to the #### -ADJUST PATHS ?/4- #### sections, and to the #### -SET PARAMETERS- #### section.<br><br>
NOTE: the name of the plot is consistently "radar_comparision.pdf", if you do not rename it and run again the script, the plot will be overwritten and **lost**.

## 6. Archive
This folder is of limited importance. It keeps some generated results and some old, now less useful scripts.
